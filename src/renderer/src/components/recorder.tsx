import { audioCapture } from '@renderer/services/audio-capture'
import { normalizeAudioBlob } from '@renderer/services/normalizer'
import { windowAudioCapture } from '@renderer/services/system-audio'
import { useState } from 'react'
import { Mic } from 'lucide-react'
import { BoxMotion } from './ui/motion'
import { usePopoverContext } from '@renderer/contexts/popover'

export function Recorder() {
  const [isRecording, setIsRecording] = useState(false)
  const { setCurrentTip, setIsProcessing } = usePopoverContext()

  const handleUserStop = async () => {
    const audioBlob = await audioCapture.stop()
    setIsRecording(false)

    // Aqui voc√™ pode enviar o `audioBlob` para a API ou salvar localmente
    console.log('√Åudio capturado:', audioBlob)
    const arrayBuffer = await audioBlob.arrayBuffer()
    const transcript = await window.api.audio.transcribe(arrayBuffer)
    console.log('Transcri√ß√£o do √°udio do usu√°rio:', transcript)

    return transcript
    // // Exemplo: transformar em URL e tocar o √°udio
    // const url = URL.createObjectURL(audioBlob)
    // const audio = new Audio(url)
    // audio.play()
    // const
  }

  const handleMediaStop = async () => {
    const audioBlob = await windowAudioCapture.stop()
    setIsRecording(false)
    const normalizedBlob = await normalizeAudioBlob(audioBlob)
    // Aqui voc√™ pode transcrever o √°udio como j√° fez antes!
    console.log('üéß √Åudio capturado:', normalizedBlob)
    const arrayBuffer = await normalizedBlob.arrayBuffer()
    const transcript = await window.api.audio.transcribe(arrayBuffer)
    console.log('Transcri√ß√£o do √°udio da m√≠dia:', transcript)

    return transcript
  }

  const handleStop = async () => {
    try {
      setCurrentTip('')
      setIsProcessing(true)
      const [userTranscript, mediaTranscript] = await Promise.all([
        handleUserStop(),
        handleMediaStop()
      ])

      const tip = await window.api.tips.generate({
        mediaPrompt: mediaTranscript,
        userPrompt: userTranscript
      })
      setCurrentTip(tip)
    } finally {
      setIsProcessing(false)
      console.log('Recording stopped')
    }
  }

  const toggle = async () => {
    console.log('Toggling recording state:', isRecording)
    if (isRecording) return handleStop()

    await audioCapture.start()
    console.log('User audio capture started')
    await windowAudioCapture.start()

    setIsRecording(true)
  }

  return (
    <BoxMotion
      w="24px"
      h="24px"
      bg="#007AFF"
      borderRadius="50%"
      display="flex"
      alignItems="center"
      justifyContent="center"
      cursor="pointer"
      style={{
        // @ts-ignore electron needs this
        WebkitAppRegion: 'no-drag'
      }}
      onClick={() => {
        toggle()
      }}
      animate={
        isRecording
          ? { backgroundColor: ['#ff0000', '#ffffff', '#ff0000'] }
          : { backgroundColor: '#007AFF' }
      }
      transition={
        isRecording ? { repeat: Infinity, duration: 1.2, ease: 'easeIn', repeatType: 'loop' } : {}
      }
    >
      <Mic size={14} />
      {isRecording ?? 'sim'}
    </BoxMotion>
    // <Flex w="full" justify={'center'} mt={10}>
    //   {!isRecording ? (
    //     <ButtonGlow onClick={handleStart}>üéôÔ∏è Iniciar Grava√ß√£o</ButtonGlow>
    //   ) : (
    //     <button onClick={handleStop}>‚èπÔ∏è Parar Grava√ß√£o</button>
    //   )}
    //   {!!userTranscript && <Flex>Usuario: {userTranscript}</Flex>}
    //   {!!mediaTranscript && <Flex>Media: {mediaTranscript}</Flex>}
    //   {!!tip && (
    //     <Flex>
    //       <h3>Tip:</h3>
    //       <p>{tip}</p>
    //     </Flex>
    //   )}
    // </Flex>
  )
}
